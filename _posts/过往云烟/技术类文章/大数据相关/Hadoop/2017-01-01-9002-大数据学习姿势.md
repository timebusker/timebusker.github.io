---
layout:     post
title:      大数据学习姿势 
date:       2018-06-15
author:     timebusker
header-img: img/home-bg.jpg
catalog: true
tags:
    - Hadoop
---

> 大数据学习姿势：循序渐进....

### 前言
#### 三个发展方向  
 - 平台搭建、优化、运维、监控
 - 大数据开发、设计、架构
 - 数据分析、挖掘
 
#### 大数据的4大特征
 - 数据量大：TB->PB 
 - 数据类型繁多：结构化、非结构化文本、日志、视频、图片、地理位置等
 - 商业价值高：但是这种价值需要在海量数据之上，通过数据分析与机器学习更快速的挖掘出来
 - 处理时效性高：海量数据的处理需求不再局限在离线计算当中
 
#### 开源大数据框架(常见)  
 - 文件存储：Hadoop HDFS、Tachyon、KFS
 - 离线计算：Hadoop MapReduce、Spark
 - 流式、实时计算：Storm、Spark Streaming、S4、Heron
 - K-V、NOSQL数据库：HBase、Redis、MongoDB
 - 资源管理：YARN、Mesos
 - 日志收集：Flume、Scribe、Logstash、Kibana
 - 消息系统：Kafka、StormMQ、ZeroMQ、RabbitMQ
 - 查询分析：Hive、Impala、Pig、Presto、Phoenix、SparkSQL、Drill、Flink、Kylin、Druid
 - 分布式协调服务：Zookeeper
 - 集群管理与监控：Ambari、Ganglia、Nagios、Cloudera Manager
 - 数据挖掘、机器学习：Mahout、Spark MLLib
 - 数据同步：Sqoop
 - 任务调度：Oozie
 
### 学习姿势  
#### 初识Hadoop  
 - 搭建Hadoop，能让它跑起来就行。建议先使用安装包命令行安装，不要使用管理工具安装，搞清楚Hadoop核心组件(原理)。
``` 
Hadoop 1.0、Hadoop 2.0
MapReduce、HDFS
NameNode、DataNode
JobTracker、TaskTracker
Yarn、ResourceManager、NodeManager
```  
 - HDFS目录操作命令;上传、下载文件命令;提交运行MapReduce示例程序;打开Hadoop WEB界面，查看Job运行状态，查看Job运行日志。
 
 - 编写MapReduce程序，打包运行。
 
#### 更高效的WordCount
 - SQL版WordCount：`SELECT word,COUNT(1) FROM wordcount GROUP BY word;`
 - SQL On Hadoop之Hive
 `The Apache Hive data warehouse software facilitates reading, writing, and managing large datasets residing 
 in distributed storage and queried using SQL syntax.`   
 为什么说Hive是数据仓库工具，而不是数据库工具呢?有的朋友可能不知道数据仓库，数据仓库是逻辑上的概念，底层使用的是数据库。
 数据仓库中的数据有这两个特点：最全的历史数据(海量)、相对稳定的;所谓相对稳定，指的是数据仓库不同于业务系统数据库，数据经常会被更新，
 数据一旦进入数据仓库，很少会被更新和删除，只会被大量查询。而Hive，也是具备这两个特点，
 因此，Hive适合做海量数据的数据仓库工具，而不是数据库工具。
 
 - 安装配置Hive 
 - 使用Hive
 - Hive是怎么工作的
 - 学会Hive的基本命令
 
#### 上传数据到Hadoop 
 - HDFS 命令
 - HDFS API
 实际环境中一般自己较少编写程序使用API来写数据到HDFS，通常都是使用其他框架封装好的方法。比如：Hive中的INSERT语句，Spark中的saveAsTextfile等。建议了解原理，会写Demo。
 - Sqoop 
 Sqoop是一个主要用于Hadoop/Hive与传统关系型数据库，Oracle、MySQL、SQLServer等之间进行数据交换的开源框架。
 就像Hive把SQL翻译成MapReduce一样，Sqoop把你指定的参数翻译成MapReduce，提交到Hadoop运行，
 完成Hadoop与其他数据库之间的数据交换。   
 使用Sqoop完成从MySQL同步数据到HDFS;使用Sqoop完成从MySQL同步数据到Hive表;如果后续选型确定使用Sqoop作为数据交换工具，
 那么建议熟练掌握，否则，了解和会用Demo即可。
 - Flume
   Flume是一个分布式的海量日志采集和传输框架，因为“采集和传输框架”，
   所以它并不适合关系型数据库的数据采集和传输。Flume可以实时的从网络协议、
   消息系统、文件系统采集日志，并传输到HDFS上。
 - 阿里开源的DataX 
 非常好用
 
#### 到处Hadoop数据到关系型数据库中
Hive和MapReduce进行分析了。那么接下来的问题是，分析完的结果如何从Hadoop上同步到其他系统和应用中去呢?其实，此处的方法和第三章基本一致的。
 
#### 快一点吧，我的SQL
其实大家都已经发现Hive后台使用MapReduce作为执行引擎，实在是有点慢。因此SQL On Hadoop的框架越来越多，
按我的了解，最常用的按照流行度依次为SparkSQL、Impala和Presto.这三种框架基于半内存或者全内存，
提供了SQL接口来快速查询分析Hadoop上的数据。
 
我们目前使用的是SparkSQL，至于为什么用SparkSQL，原因大概有以下吧：使用Spark还做了其他事情，
不想引入过多的框架;Impala对内存的需求太大，没有过多资源部署。 
 
 - 关于Spark和SparkSQL
 什么是Spark，什么是SparkSQL。  
 Spark有的核心概念及名词解释。  
 SparkSQL和Spark是什么关系，SparkSQL和Hive是什么关系。   
 SparkSQL为什么比Hive跑的快。   
 - 如何部署和运行SparkSQL
 Spark有哪些部署模式?   
 如何在Yarn上运行SparkSQL?   
 使用SparkSQL查询Hive中的表。Spark不是一门短时间内就能掌握的技术，因此建议在了解了Spark之后，可以先从SparkSQL入手，循序渐进。  
 关于Spark和SparkSQL，如果你认真完成了上面的学习和实践，此时，你的”大数据平台”应该是这样的。   

#### 消息队列 
在实际业务场景下，特别是对于一些监控日志，想即时的从日志中了解一些指标(关于实时计算，后面章节会有介绍)，这时候，从HDFS上分析就太慢了，尽管是通过Flume采集的，但Flume也不能间隔很短就往HDFS上滚动文件，这样会导致小文件特别多。   

为了满足数据的一次采集、多次消费的需求，这里要说的便是Kafka。  

 - 关于Kafka
 什么是Kafka?Kafka的核心概念及名词解释。  
 - 如何部署和使用Kafka  
 使用单机部署Kafka，并成功运行自带的生产者和消费者例子。使用Java程序自己编写并运行生产者和消费者程序。
 Flume和Kafka的集成，使用Flume监控日志，并将日志数据实时发送至Kafka。  
 
#### 越来越多的分析任务  
不仅仅是分析任务，数据采集、数据交换同样是一个个的任务。这些任务中，有的是定时触发，有点则需要依赖其他任务来触发。
当平台中有几百上千个任务需要维护和运行时候，仅仅靠crontab远远不够了，这时便需要一个调度监控系统来完成这件事。
调度监控系统是整个数据平台的中枢系统，类似于AppMaster，负责分配和监控任务。  

 - Apache Oozie  
 Oozie是什么?有哪些功能?  
 Oozie可以调度哪些类型的任务(程序)?  
 Oozie可以支持哪些任务触发方式?  
 安装配置Oozie。  
 
 - 其他开源的任务调度系统
 Azkaban，light-task-scheduler，Zeus，等等。另外，我这边是之前单独开发的任务调度与监控系统，
 具体请百度《大数据平台任务调度与监控系统》。   
 
#### 实时数据  
介绍Kafka的时候提到了一些需要实时指标的业务场景，实时基本可以分为绝对实时和准实时，
绝对实时的延迟要求一般在毫秒级，准实时的延迟要求一般在秒、分钟级。
对于需要绝对实时的业务场景，用的比较多的是Storm，对于其他准实时的业务场景，
可以是Storm，也可以是Spark Streaming。当然，如果可以的话，也可以自己写程序来做。    

 - Storm
 什么是Storm?有哪些可能的应用场景?  
 Storm由哪些核心组件构成，各自担任什么角色?  
 Storm的简单安装和部署。  
 自己编写Demo程序，使用Storm完成实时数据流计算。  
 
 - Spark Streaming  
 什么是Spark Streaming，它和Spark是什么关系?   
 Spark Streaming和Storm比较，各有什么优缺点?   
 使用Kafka + Spark Streaming，完成实时计算的Demo程序。  
 
至此，你的大数据平台底层架构已经成型了，其中包括了数据采集、数据存储与计算(离线和实时)、数据同步、任务调度与监控这几大模块。
接下来是时候考虑如何更好的对外提供数据了。
 

#### 数据要对外服务  
通常对外(业务)提供数据访问，大体上包含以下方面。  

离线：比如，每天将前一天的数据提供到指定的数据源(DB、FILE、FTP)等;离线数据的提供可以采用Sqoop、DataX等离线数据交换工具。   

实时：比如，在线网站的推荐系统，需要实时从数据平台中获取给用户的推荐数据，这种要求延时非常低(50毫秒以内)。根据延时要求和实时数据的查询需要，可能的方案有：HBase、Redis、MongoDB、ElasticSearch等。   

OLAP分析：OLAP除了要求底层的数据模型比较规范，另外，对查询的响应速度要求也越来越高，可能的方案有：Impala、Presto、SparkSQL、Kylin。如果你的数据模型比较规模，那么Kylin是最好的选择。   

即席查询：即席查询的数据比较随意，一般很难建立通用的数据模型，因此可能的方案有：Impala、Presto、SparkSQL。   

#### 机器学习
分类问题：包括二分类和多分类，二分类就是解决了预测的问题，就像预测一封邮件是否垃圾邮件;多分类解决的是文本的分类;  

聚类问题：从用户搜索过的关键词，对用户进行大概的归类。  

推荐问题：根据用户的历史浏览和点击行为进行相关推荐。   

大多数行业，使用机器学习解决的，也就是这几类问题。入门学习线路，数学基础;机器学习实战，懂Python最好;SparkMlLib提供了一些封装好的算法，以及特征处理、特征选择的方法。
 
 
 
 
 
 
 
 
 
 
 
 
 
 