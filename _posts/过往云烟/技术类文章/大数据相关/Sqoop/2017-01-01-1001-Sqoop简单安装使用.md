---
layout:     post
title:      Sqoop简单安装使用
date:       2018-06-02
author:     timebusker
header-img: img/home-bg.jpg
catalog: true
tags:
    - Sqoop
---

#### 概述
Sqoop 是apache旗下一款`Hadoop和关系数据库服务器之间传送数据的**工具**`。核心的功能有两个：

- `导入数据`：MySQL，Oracle 导入数据到 Hadoop 的 HDFS、HIVE、HBASE 等数据存储系统
- `导出数据`：从 Hadoop 的文件系统中导出数据到关系数据库 mysql 等 Sqoop 的本质还是一个命令行工具，和 HDFS，Hive 相比，并没有什么高深的理论。

![Sqoop简单安装使用](img/older/sqoop/1.png)

> 工作机制

将导入或导出命令翻译成`MapReduce`程序来实现，在翻译出的`MapReduce`中主要是对`InputFormat`和`OutputFormat`进行定制。

#### 安装
Sqoop就是一个工具， 只需要在一个节点上进行安装即可。在使用的时候有可能会与`HDFS`、`MapReduce`、`YARN`、`ZooKeeper`、`Hive`、`HBase`、`MySQL`等打交道。

> 如果你的Sqoop工具将来要进行Hive或者Hbase等等的系统和MySQL之间的交互,你安装的Sqoop软件的节点`有对应服务的客户端连接程序`。

> 1.4.7版本以下为Sqoop1，1.99.4以上为Sqoop2，`绝大部分企业所使用的sqoop的版本都是 Sqoop1`

##### 修改配置文件
- 修改 sqoop-env.sh，配置服务组件

```
cp sqoop-env-template.sh  sqoop-env.sh

# Set Hadoop-specific environment variables here.

#Set path to where hadoop-*-core.jar is available
export HADOOP_MAPRED_HOME=/root/hadoop-2.7.5

#set the path to where bin/hbase is available
export HBASE_HOME=/root/hbase-1.2.6

#Set the path to where bin/hive is available
export HIVE_HOME=/root/apache-hive-2.3.3-bin

#Set the path for where zookeper config dir is
export ZOOCFGDIR=/root/zookeeper-3.4.10/conf
```

- 配置`$SQOOP_HOME`系统环境变量

- 增加MySQL/Oracle等对应的依赖包，拷贝到`$SQOOP_HOME/lib`目录下

#### Sqoop操作
- 查看数据库

```
sqoop list-databases \
    --connect jdbc:mysql://hadoop1:3306/ \
    --username root \
    --password root
```

- `指定数据库`查看数据表

```
sqoop list-tables \
    --connect jdbc:mysql://hadoop1:3306/mysql \
    --username root \
    --password root
```

- 创建与MySQL一样的Hive表

```
sqoop create-hive-table \
    --connect jdbc:mysql://hadoop1:3306/mysql \
    --username root \
    --password root \
    --table help_keyword \
    --hive-table hk
```

##### 数据导入HDFS
`导入工具`导入单个表从`RDBMS(关系型数据库)`到`HDFS`。表中的每一行被视为`HDFS`的记录。所有记录都存储为`文本/Avro/sequence`等类型的数据文件。

> 语法格式

```
sqoop import \
--connect <jdbc-uri> jdbc 连接地址 \
--connection-manager <class-name> 连接管理者  \
--driver <class-name> 驱动类  \
--hadoop-mapred-home <dir> $HADOOP_MAPRED_HOME \
--help help 信息 \
-P 从命令行输入密码  \
--password <password> 密码  \
--username <username> 账号  \
--verbose 打印流程信息  \
--connection-param-file <filename> 可选参数 \
``` 

- 普通导入，`导入的默认路径：/user/hadoop/`

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
-m 1
```

- 指定分隔符和导入路径

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--target-dir /user/hadoop11/my_help_keyword1 \
--fields-terminated-by '\t' \
-m 2
```

- 带where条件

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--where "name='STRING' " \
--table help_keyword \
--target-dir /sqoop/hadoop11/myoutport1 \
-m 1
```

- 查询指定列

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--columns "name" \
--where "name='STRING' " \
--table help_keyword \
--target-dir /sqoop/hadoop11/myoutport22 \
-m 1
```

- 指定自定义查询SQL

- `引号问题`
   + 外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 
   + 外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义

- 自定义的SQL语句中必须带有WHERE \$CONDITIONS

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/ \
--username root \
--password root \
--target-dir /user/hadoop/myimport33_1 \
--query 'select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = "STRING"' \
--split-by  help_keyword_id \
--fields-terminated-by '\t' \
-m 4
```

##### 数据导入Hive
Sqoop导入关系型数据到hive的过程是`先导入到hdfs，然后再load进入hive`

- 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名：

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--hive-import \
-m 1
```

- 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录

>  sqoop会自动给创建hive的表，但是不会自动创建不存在的库

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--fields-terminated-by "\t" \
--lines-terminated-by "\n" \
--hive-import \
--hive-overwrite \
--create-hive-table \
--delete-target-dir \
--hive-database  default \
--hive-table new_help_keyword
```

等价于：

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--fields-terminated-by "\t" \
--lines-terminated-by "\n" \
--hive-import \
--hive-overwrite \
--create-hive-table \ 
--hive-table  default.new_help_keyword \
--delete-target-dir
```

- 增量导入

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--target-dir /user/hadoop/myimport_add \
--incremental  append \
--check-column  help_keyword_id \
--last-value 500 \
-m 1
```

##### 数据导入Hbase

```
sqoop import \
--connect jdbc:mysql://hadoop1:3306/mysql \
--username root \
--password root \
--table help_keyword \
--hbase-table new_help_keyword \
--column-family person \
--hbase-row-key help_keyword_id
```

#### 数据导出

```

```
