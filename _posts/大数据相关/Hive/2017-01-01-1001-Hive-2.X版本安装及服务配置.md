---
layout:     post
title:      Hive-2.X版本安装及服务配置
date:       2017-12-11
author:     timebusker
header-img: img/home-bg.jpg
header-img: img/taylorswift/post-bg-swift.jpg
catalog: true
tags:
    - Hive
---

### 相关概念
#### Hive
Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上，总归为大数据，并使得查询和分析方便。
并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。

#### 数据仓库
是能够为企业的各个级别的决策提供数据支撑的数据——简单理解，存放数据的地方   
- **数据库和数据仓库之间的区别:**  
   - 现代数据仓库，是**构建在数据库原理之上**的，使用**类似**数据库作为载体存放数据。  
   - **数据仓库**着重强调的是存放的**历史数据**，**数据库**着重强调的是存放**在线的数据（及时数据）**。  
   - **数据仓库**着重强调的是`OLAP`的操作，**数据库**着重强调的是`OLTP`的操作   
     > OLAP：Online Analysis Processing 联机分析处理--->对数据进行**查询分析**：`select、load `,hive能够根据设置分区、线程数控制，能在一定程度上实现事务处理，但是性能很低。         
     > OLTP：Online Transcation Processing 联机事务处理--->对数据进行**事务性操作**：`update delete`        

#### ETL
ETL是用来构建我们一个数据仓库的概念.
> **E(Extract 提取):** 获取数据的过程，就称之为提取，采集      
> **T(Transform 转化):** 对进入仓库的数据进行分类、清洗       
> **L(Load 加载):** 数据进入仓库的过程就是Load       

### HIve介绍
Hive是一款SQL的解析引擎，能够将HQL转移成为MR在hadoop计算HDFS上面的数据。简单来说，就是hadoop客户端。     

#### 存储结构
- Hive的数据存储基于`Hadoop HDFS`
- Hive**没有专门的数据存储格式**
- 存储结构主要包括：数据库、文件、表、视图、索引
- Hive默认可以直接加载文本文件（`TextFile`），还支持`SequenceFile`、`RCFile` 、`ORCFile`、`Parquet`等压缩文件
- 创建表时，指定Hive数据的列分隔符与行分隔符，Hive即可解析数据

#### 系统架构
![image](https://raw.githubusercontent.com/timebusker/timebusker.github.io/master/img/hive/0.png?raw=true)  
![image](https://raw.githubusercontent.com/timebusker/timebusker.github.io/master/img/hive/2.png?raw=true)  
![image](https://raw.githubusercontent.com/timebusker/timebusker.github.io/master/img/hive/1.png?raw=true)  
![image](https://raw.githubusercontent.com/timebusker/timebusker.github.io/master/img/hive/3.png?raw=true)  

### Hive安装
#### 安装JDK
提供JVM运行环境     

#### [安装MySQL](http://www.timebusker.top/2018/05/04/MySQL-%E4%B8%80-MySQL%E5%AE%89%E8%A3%85/)
利用MySQL存储元数据信息。     

#### [安装Hadoop](http://www.timebusker.top/2018/05/19/2005-Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Hadoop%E9%9B%86%E7%BE%A4-HA-%E6%9C%8D%E5%8A%A1%E6%90%AD%E5%BB%BA/)
- 利用 HDFS 进行存储   
- 利用 MapReduce 进行计算

#### 安装Hive

```
# 进入$HIVE_HOME/conf/修改文件
cp  hive-env.sh.template  hive-env.sh
cp  hive-default.xml.template  hive-site.xml

# 编辑hive-env.sh,配置环境变量
export JAVA_HOME=/usr/java/jdk1.8.0_111    
export HADOOP_HOME=/root/hadpoop-2.8.1    
export HIVE_HOME=/root/hive-2.3.4   

# 编辑hive-site.xml
# 配置元数据库
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://hdp-cluster-6:3306/hive_a?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
</property>
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>timebusker</value>
</property>
<!-- 指定hive查询日志本地存放目录  -->  
<property>
    <name>hive.querylog.location</name>
    <value>/BDS2/hive/querylog</value>
</property>
<!-- 指定Hive的DDL/DML作业计算结果本地存储目录 -->  
<property>
    <name>hive.exec.local.scratchdir</name>
    <value>/BDS2/hive/scratchdir</value>
</property>
<!-- 用于向远程文件系统添加资源的本地临时目录  -->
<property>
    <name>hive.downloaded.resources.dir</name>
    <value>/BDS2/hive/downloaded</value>
</property>
<!-- 指定HDFS内hive数据存放目录,HDFS自动创建 --> 
<!-- 默认：/user/hive/warehouse -->  
<property>
	<name>hive.metastore.warehouse.dir</name>  
	<value>/user/hive_a/warehouse</value>   
</property>
<!-- 指定任务队列，默认default --> 
<property>
	<name>mapreduce.job.queuename</name>  
	<value>QueueA</value>   
</property>
<!-- 关闭源数据库版本校验机制 --> 
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
</property>
</configuration>

# 拷贝元数据库的驱动程序
cp mysql-connector-java-5.1.39.jar $HIVE_HOME/lib/

# 初始化hive元数据仓库
$HIVE_HOME/bin/schematool -initSchema -dbType mysql -userName root -passWord timebusker

# 启动hive
$HIVE_HOME/bin/hive

# 提交作业前设置调度队列
set mapreduce.job.queuename=Queue_A
```

#### 元数据库编码格式

```
# 将有关中文字段修改字符编码设置

ALTER TABLE COLUMNS_V2 MODIFY COLUMN COMMENT VARCHAR(256) CHARACTER SET utf8;
ALTER TABLE TABLE_PARAMS MODIFY COLUMN PARAM_VALUE VARCHAR(4000) CHARACTER SET utf8;
ALTER TABLE PARTITION_PARAMS MODIFY COLUMN PARAM_VALUE VARCHAR(4000) CHARACTER SET utf8;
ALTER TABLE PARTITION_KEYS MODIFY COLUMN PKEY_COMMENT VARCHAR(4000) CHARACTER SET utf8;
ALTER TABLE INDEX_PARAMS MODIFY COLUMN PARAM_VALUE VARCHAR(4000) CHARACTER SET utf8;
```

### Hive运行及访问方式
#### CLI--Hive的shell环境

```

# hive 会自动运行$HOME/.hiverc 文件
# 例如当前用户是 root,可以在/root/目录下，创建一个.hiverc 文件，内容如下：
set hive.cli.print.header=true;
set hive.cli.print.current.db=true;
set hive.exec.mode.local.auto=true;
# Hive 会将最近 100 行命令记录到$HOME/.hivehistory

# 启动单机版Hive程序,改启动方式可以进入Hive的shell环境  
./$HIVE_HOME/bin/hive   

# 在shell环境下引用执行本地sql脚本，可结合crontab 做定时任务调度
hive -f hive.hql
# 执行语句
hive -e "set hive.exec.mode.local.auto=true;select * from test;"
# 或者（针对终端命令，可以使用--hiveconf指定额外的配置信息）
hive -e "select * from test;" --hiveconf hive.exec.mode.local.auto=true

# 同样，在hive终端环境下，也可以引用本地文件执行
source hive.hql 
```

#### JDBC服务

```
# hive 启动 hive Thrift 服务端 (默认端口 10000,可通过 hive.server2.thrift.port 参数调整)
hive --service hiveserver2

# 启动时指定端口
hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10016

# 后台启动
nohup ./hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10002 1>/dev/null 2>&1 &

# 在该服务下，可以使用beeline客户端连接
# 此方法在hive 1.X和2.X中有差异，hive2.x强制使用用户名和密码验证，且需要手动实现验证接口。
# http://www.cnblogs.com/zuizui1204/p/9999652.html
# https://www.cnblogs.com/lijinze-tsinghua/p/8563054.html
# http://lxw1234.com/archives/2016/05/675.htm
# http://lxw1234.com/archives/2016/01/600.htm
beeline -u jdbc:hive2://hdp-cluster-11:10016  （1.X）
beeline -u jdbc:hive2://hdp-cluster-11:10016 root timebusker （2.X）
```
#### WebGUI 的方式
WebGUI需要构建`war包`到hive环境中启动，实际工作中很少用到（待续）

### Hive 常设置参数

```
显示列名称
set hive.cli.print.header=true; 

显示数据库名称
set hive.cli.print.current.db=true;

本地模式
set hive.exec.mode.local.auto=true;

开启分桶
set hive.enforce.bucketing=true;

开启动态分区支持
set hive.exec.dynamic.partition=true;

动态分区的模式，默认 strict，
strict    表示必须指定至少一个分区为静态分区，
nonstrict 模式表示允许所有的分区字段都可以使用动态分区。
set hive.exec.dynamic.partition.mode=nonstrict;

在每个执行 MR 的节点上，最大可以创建多少个动态分区，如果超过了这个数量就会报错
hive.exec.max.dynamic.partitions.pernode （缺省值 100）：

在所有执行 MR 的节点上，最大一共可以创建多少个动态分区
hive.exec.max.dynamic.partitions （缺省值 1000）：

整个 MR Job 中，最大可以创建多少个 HDFS 文件
hive.exec.max.created.files （缺省值 100000）：

对分区表查询必须加分区过滤,不允许笛卡尔积查询，order by 后面必须有 limit 限制
set hive.mapred.mode=strict;

不严格模式
set hive.mapred.mode=nonstrict;

开启任务并行执行
set hive.exec.parallel=true;

开启 map join
set hive.auto.convert.join=true;
set hive.mapjoin.smalltable.filesize=25000000;

忽略 MAPJOIN 标记
set hive.ignore.mapjoin.hint=true;
```

#### 常见问题

- `Specified key was too long; max key length is 767 bytes`

> 是由于是由于元数据的编码导致

> 解决办法
    - 配置MySQL服务编码格式，可统一为utf-8
	- 启动hive之前，初始化hive元数据仓库（`按步骤指令初始化`）
	- 初始化hive元数据仓库后，hive相关表格均已建好，再设置表字段编码格式
	
- `hive Permission denied: user=root, access=WRITE, inode="/user":hdfs:supergroup:drwxr-xr-x`

> 是由于HDFS目录权限限制导致

> 解决办法
    - 目录授权：`hadoop fs -chmod 777 /user`
	
- `User root is not allowed to impersonate anonymous（root账户不允许被匿名使用）`

> 解决办法：开启Hadoop[用户授权代理ProxyUser](http://www.timebusker.top/2018/12/15/1001-HDFS%E7%89%B9%E6%80%A7-%E7%94%A8%E6%88%B7%E6%8E%88%E6%9D%83%E4%BB%A3%E7%90%86ProxyUser/)

> 通过设置root代理，支持root账户代理任何账户，包括匿名账户。


- Hive提交SQL，Yarn一直处于`ACCEPTED`

此种情况比较复杂，可能存在以下原因：
    - 集群服务有故障，需要排查所有服务是否正常；
	- 账户权限不足
	
- `MetaException(message:Hive Schema version 2.3.0 does not match metastore's schema version 1.2.0 Metastore is not upgraded or corrupt)`

> 在使用`./hive` 启动hive服务时的异常，是由于Hive开启元数据版本校验机制，可以关闭校验或者手动更改

> 关闭校验

```
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
</property>
```
